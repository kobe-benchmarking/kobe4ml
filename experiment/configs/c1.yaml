metadata:
  experiment: kn7fej4o
  date: 06/03/2025
  id: c1
  name: BitBrain-LSTM-Autoencoder
  description: A paragraph-long description of this run. Here, this run applies an LSTM architecture that classifies timeseries into categorical classes on the BitBrain dataset.

implementation:
    module: lstm_ae

run:
  - id: kn7fej4o_01
    type: training

    model: 
      url: s3://manolo-data/models/lstm_ae.pth
      
      parameters:
        num_feats: 2
        latent_seq_len: 1 
        latent_num_feats: 8 
        hidden_size: 4
        num_layers: 1
        dropout: 0.05

    parameters:
      batch_size: 512
      seq_len: 240
      loss: BlendedLoss
      epochs: 1000
      patience: 30
      lr: 0.0001
      optimizer: Adam
      scheduler:
        name: ReduceLROnPlateau
        params: 
          factor: 0.99
          patience: 3

    dataset:
      url: s3://manolo-data/datasets/bitbrain-ds/
    
    metrics:
      - epochs
      - train_time
      - best_train_loss
      - best_val_loss
      - mae
      - mse

    id: kn7fej4o_02
    type: inference

    model: 
      url: s3://manolo-data/models/lstm_ae.pth
      
      parameters:
        num_feats: 2
        latent_seq_len: 1 
        latent_num_feats: 8 
        hidden_size: 4
        num_layers: 1
        dropout: 0.05

    parameters:
      batch_size: 512
      seq_len: 240
      loss: BlendedLoss

    dataset:
      url: s3://manolo-data/datasets/bitbrain-ds/
    
    metrics:
      - mae
      - mse

    # TBD
    # metrics:
    #   - module: mae
    #   - module: mse